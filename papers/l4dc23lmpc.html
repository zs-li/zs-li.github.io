---
layout: page
title: ! 'Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming Controllers Inspired by Model Predictive Control'
comments: true
---
<hr />
<p>
<b>Yiwen Lu, Zishuo Li, Yihan Zhou, Na Li, Yilin Mo</b>
</p>

<p>
<a href="https://l4dc.web.ox.ac.uk/home"><i>6th Annual Learning for Dynamics & Control Conference</i></a>, <a href="https://arxiv.org/abs/2312.05332">Available Online.</a>
</p> 

<!-- <p>
doi: 10.1109/CDC45484.2021.9682830.
</p> -->

<p>
Link to the <a href="../../../public/papers/l4dc23lmpc.pdf">paper pdf</a>
</p>

<!--<p>
If you want to leave any comments, you can annotate the <a href="../../../pdfviewer/viewer/web/viewer.html?file=%2Fpublic%2Fpapers%2Fcdc21_arxiv.pdf">pdf</a>. I will try to be responsive. You can also annotate this page or leave comments below.
</p> -->

<div id="outline-container-orgeb2596f" class="outline-2">
<h2 id="orgeb2596f">Abstract</h2>
<div class="outline-text-2" id="text-orgeb2596f">
<p>
    In this paper, we introduce a new class of parameterized controllers, drawing inspiration from Model Predictive Control (MPC). These controllers adopt a Quadratic Programming (QP) structure similar to linear MPC, with problem parameters being learned rather than derived from models. This approach may address the limitations of commonly learned controllers with Multi-Layer Perceptron (MLP) architecture in deep reinforcement learning, in terms of explainability and performance guarantees. The learned controllers not only possess verifiable properties like persistent feasibility and asymptotic stability akin to MPC, but they also empirically match MPC and MLP controllers in control performance. Moreover, they are more computationally efficient in implementation compared to MPC and require significantly fewer learnable policy parameters than MLP controllers. Practical application is demonstrated through a vehicle drift maneuvering task, showcasing the potential of these controllers in real-world scenarios.
</p>
</div>
</div>
